---
title: "Early outbreak and Generation time"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Early outbreak and Generation time}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

```{r setup, include=FALSE}

# bibliography: ../references.bib
# link-citations: TRUE

options(
  repos = c(
    "ai4ci" = 'https://ai4ci.r-universe.dev/',
    "terminological" = 'https://terminological.r-universe.dev/',
    CRAN = 'https://cloud.r-project.org'
  )
)

future::plan(future::multisession, workers = parallel::detectCores()-2)

library(tidyverse)
library(tidyabc)

knitr::opts_chunk$set(
  echo = TRUE,
  dev = "ragg_png",
  fig.width = 7,
  fig.height = 5
)

ggplot2::set_theme(theme_minimal())
```

## Introduction

This vignette demonstrates a complex, real-world application of ABC to infer key
epidemiological parameters from early outbreak data. Specifically, we aim to
estimate the **basic reproduction number (R0)** and the **generation time
distribution** using only linked case data that includes symptom onset times and
observation delays.

During the early stages of an outbreak, detailed information like who infected
whom (a "transmission tree") is often incomplete or unknown. However, if we can
observe *some* linked transmission pairs (e.g., through contact tracing), we can
use the times between symptom onsets in these pairs—the **serial interval**—to
learn about the underlying **generation time** (the time between infection in a
primary case and infection in a secondary case).

This is a challenging inference problem because:
1.  **Infection times are hidden**: 
We only observe symptom onset and reporting times, which are delayed and stochastic.
2.  **Observation is biased**: 
Cases are only observed if their symptom onset and reporting happen within the
observation window. This "right-censoring" distorts the observed distributions.
3.  **Parameters are linked**: 
R0 is mathematically related to the growth rate (`r0`) and the generation time
distribution via the **Wallinga-Lipsitch equation**.

`tidyabc` provides a flexible framework to build a simulation model that
captures this complexity and then use ABC to infer the hidden parameters.

---

## Simulation

`tidyabc` includes a synthetic dataset generated from a branching process model
which we are using for this example (`sim_outbreak`) which is supposed to 
replicate the early days of an infectious disease outbreak where very little is 
known about the pathogen and its epidemiological parameters.

```{r}

sim_params = sim_outbreak$parameters
observed = sim_outbreak$contact_tracing

R0_truth = sprintf("%1.1f", sim_params$R0)
r0_truth = sprintf("%1.2f", sim_params$r0) 
gt_truth = sprintf("%1.1f \u00B1 %1.1f", sim_params$mean_gt, sim_params$sd_gt)
onset_truth = sprintf("%1.1f \u00B1 %1.1f", sim_params$mean_onset, sim_params$sd_onset)
obs_truth = sprintf("%1.1f \u00B1 %1.1f", sim_params$mean_obs, sim_params$sd_obs)

```

1.  **Branching Process**: 

The core outbreak is simulated as a stochastic branching process with a constant
`R0` of `r R0_truth` and with a gamma distributed generation time with mean 
and SD of `r gt_truth`, together these imply an initial growth rate of `r r0_truth`. 

After infection a proportion of people experience 
symptoms with a random delay with mean and SD of `r onset_truth`. Of those with 
symptoms a proportion are detected with a random delay with mean and SD of 
`r obs_truth`. The outbreak is simulated to day `r sim_params$T_obs`, after which 
point no further cases can be observed.

### The Observed Data

From this simulated outbreak, we extract three key pieces of information to use
as our observational data (`obsdata`) for ABC:

1.  **Primary Case Onset Times**:

```{r fig.width=3, fig.height=3}

ggplot(observed, aes(x=onset_time))+geom_histogram(binwidth = 1)
```

This histogram shows the distribution of symptom onset times for all observed
primary cases. The shape is influenced by the exponential growth rate of the
outbreak, the symptom onset delay distribution, and the delay to observation.

## Delay to observation

```{r fig.width=3, fig.height=3}

# Data

delay_distribution = observed %>% dplyr::transmute(
  obs_delay = obs_time - onset_time
)

ggplot(delay_distribution, aes(x = obs_delay))+geom_histogram(binwidth = 1)+
  xlab("symptom to observation")
```

This shows the distribution of delays between symptom onset and when the case
was observed. This reflects the `mean_obs` and `sd_obs` parameters. In an
exponentially growing outbreak longer delays to observation may be be fully
represented due to right censoring.

## Observed serial interval

```{r fig.width=3, fig.height=3}
serial_pairs = observed %>%
  inner_join(
    observed,
    by = c("id" = "contact_id"),
    suffix = c(".1", ".2")
  ) %>%
  transmute(
    serial_interval = onset_time.2 - onset_time.1 #order known
    # serial_interval = abs(onset_time.2 - onset_time.1) #order uncertain
  )

ggplot(serial_pairs) +
  geom_histogram(aes(x = serial_interval), binwidth = 1)+
  xlab("symptom serial interval (given observed)")
```

This is the most critical piece of data. The serial interval is the time between
symptom onsets in observed transmission pairs. Because symptom onset is itself
delayed from infection, the serial interval is a **noisy and potentially biased
proxy** for the true generation time. Our model must account for this
relationship. Longer serial intervals are not as frequently observed in the 
context of exponential growth as long intervals between cases are less likely
to have been observed yet.

```{r}
obsdata = list(
  onset = as.numeric(observed$onset_time),
  diff = as.numeric(delay_distribution$obs_delay),
  si = as.numeric(serial_pairs$serial_interval)
)
```


## The Inference Model

Our goal is to fit a model that can simultaneously explain all three observed 
data components. The model makes explicit assumptions about the hidden processes:

### Model Assumptions

1.  **Transmission Dynamics**: 
Infection times follow a process of **constant exponential growth** with rate $r_0$.
2.  **Delays**:
    - Time from infection to symptom onset is **Gamma-distributed** ($\mu_{onset}$, $\sigma_{onset}$).
    - Time from symptom onset to observation is **Gamma-distributed** ($\mu_{obs}$, $\sigma_{obs}$).
    - The true **generation time** (time between infections in a pair) is **Gamma-distributed** ($\mu_{gt}$, $\sigma_{gt}$).

$$
\begin{align}
t_{max} - T_{inf} &\sim Exp(r_0) \\
\Delta T_{inf \rightarrow onset} &\sim Gamma(\mu_{onset},\sigma_{onset}) \\
\Delta T_{onset \rightarrow obs} &\sim Gamma(\mu_{obs},\sigma_{obs}) \\
\Delta T_{gt} &\sim Gamma(\mu_{gt},\sigma_{gt}) \\
\end{align}
$$

Secondary cases are generated from primary cases with a poisson process with 
rate equal to the reproduction number $R_0$ and the time of infection of secondary
cases ($T_{inf_2}$) by the generation time.:

$$
\begin{align}
T_{onset} &= T_{inf} + \Delta T_{inf \rightarrow onset} \\
T_{obs} &= T_{onset} + \Delta T_{onset \rightarrow obs}\\
N_{inf_1 \rightarrow inf_2} &\sim Poisson(R_0) \\
T_{inf_2} &= T_{inf_1} + \Delta T_{gt} \\
\Delta T_{onset_1 \rightarrow onset_2} &=  \Delta T_{gt} + \Delta T_{inf_2 \rightarrow onset_2} -  \Delta T_{inf_1 \rightarrow onset_1} \\
\end{align}
$$

    
3.  **Observation Process**: 
A primary case is only "observed" if its symptom onset is after day 0 and its
observation time is before $T_{obs}$. A secondary case is only observed if the
primary case was observed and its symptom onset is after day 0 and its 
observation time is also before $T_{obs}$.

$$
\begin{align}
O_1 &= I(t_0 \le T_{onset_1}, T_{obs_1} \le t_{max}) \\
O_{1,2} &= I(O_1, t_0 \le T_{onset_2}, T_{obs_2} \le t_{max})\\
T_{onset_1}|O_1 &\Rightarrow \text{primary case times}\\
\Delta T_{onset_1 \rightarrow obs_1}|O_1 &\Rightarrow \text{onset to interview delay}\\
\Delta T_{onset_1 \rightarrow onset_2}|O_{1,2} &\Rightarrow \text{onset to onset serial interval}\\
\end{align}
$$

4.  **Parameter Linkage**: 
The basic reproduction number **R0** is not a free parameter. It is **determined
by `r0` and the generation time distribution** through the Wallinga-Lipsitch
formula, specific for gamma distributed generation times: 

$$
\begin{align}
R_0 = (1 +  \frac{r_0\sigma_{gt}^2}{\mu_{gt}})^{\frac{\mu_{gt}^2}{\sigma_{gt}^2}}
\end{align}
$$


### The Simulation Function (`sim1_fn`)

The mathematical formulation of the model is implemented below, showing how the
hidden infection times (`T_inf`) are used to generate the observed symptom times
(`T_onset`), observation times (`T_obs`), and serial intervals (derived from
linked pairs).

This function is fully self contained and using `carrier::crate` to bind the
observation window `T_obs` and the number of primary cases `n` from the observed
data.

```{r}
n = nrow(observed)

sim1_fn = carrier::crate(
  function(r0, mean_onset, sd_onset, mean_obs, sd_obs, mean_gt, sd_gt, R0, ...) {
    
    # Primary case infection time
    # exponentially distributed in time. Need to make sure we have enough samples 
    # before t0 observation cutoff to account for early observed cases.
    
    t_early = - stats::qgamma(0.99,mean_onset,sd_onset) # t starts at 0
    t_inf_1 = tidyabc::rexpgrowth(n, r0, T_obs, t_early)
    
    onset_delay = tidyabc::rgamma2(n, mean_onset, sd_onset)
    obs_delay = tidyabc::rgamma2(n, mean_obs, sd_obs)
    
    t_onset_1 = t_inf_1 + onset_delay
    t_obs_1 = t_onset_1 + obs_delay
    
    # Primary case observations:
    # Onset after t0 and observed before T
    obs_1 = t_obs_1 < T_obs & t_onset_1 > 0
    
    t_inf_1 = t_inf_1[obs_1]
    t_onset_1 = t_onset_1[obs_1]
    t_obs_1 = t_obs_1[obs_1]
    
    n1 = length(t_inf_1)
    
    # Secondary case. Numbers of secondary cases are poission(R0). Could add 
    # dispersion here and fit it also
    # Only observed primary will be observed secondary so we can restrict to 
    # observed subset
    # browser()
    case_2ary = stats::rpois(n1,R0)
    index_1ary = rep(seq_along(case_2ary), case_2ary)
    n2 = length(index_1ary)
    
    gt_delay = tidyabc::rgamma2(n2, mean_gt, sd_gt)
    onset_delay_2 = tidyabc::rgamma2(n2, mean_onset, sd_onset)
    obs_delay_2 = tidyabc::rgamma2(n2, mean_obs, sd_obs)
    
    t_inf_2 = t_inf_1[index_1ary] + gt_delay
    t_onset_2 = t_inf_2 + onset_delay_2
    t_obs_2 = t_onset_2 + obs_delay_2
    
    # order dependent
    serial_interval = floor(t_onset_2) - floor(t_onset_1[index_1ary])
    # order independent
    # serial_interval = abs(floor(t_onset_2) - floor(t_onset_1[index_1ary]))
    
    # Secondary case observations
    obs_2 = t_obs_2 < T_obs & t_onset_2 > 0
    
    serial_interval = serial_interval[obs_2]
    t_onset_2 = t_onset_2[obs_2]
    
    return(list(
      onset = floor(t_onset_1),
      diff = floor(t_obs_1) - floor(t_onset_1),
      si = serial_interval
    ))
  },
  T_obs = sim_params$T_obs,
  n=n
)
```

It performs the following steps:

1.  **Simulate Primary Infections**: 
Generates `n` primary infection times from an exponentially growing process,
starting early enough to account for long symptom delays.
2.  **Add Delays for Primary Cases**: 
Adds symptom onset and observation delays, then applies the observation filter.
3.  **Simulate Secondary Infections**: 
For each observed primary case, it generates a Poisson(`R0`) number of secondary
cases.
4.  **Add Delays for Secondary Cases**: 
Adds their own generation time delay, symptom onset delay, and observation delay.
5.  **Calculate Observed Quantities**: 
Computes the final vectors for `onset`, `diff` (observation delay), and `si`
(serial interval) from the simulated and filtered data.

### The Scoring Function (`scorer1_fn`)

The scorer function compares the simulated output to the observed data using the
Wasserstein distance, which is well-suited for comparing distributions of event
times. It also uses the mean absolute difference between simulated and observed
data, to give some more information about the most important aspect of the 
serial interval distribution.

```{r}

scorer1_fn = function(simdata, obsdata) {
  
  onset = calculate_wasserstein(simdata$onset, obsdata$onset)
  diff = calculate_wasserstein(simdata$diff, obsdata$diff)
  si = calculate_wasserstein(simdata$si, obsdata$si)
  mad_si = abs(mean(simdata$si) - mean(obsdata$si))
  
  return(list(
    sim_onset = onset,
    sim_diff = diff,
    sim_si=si,
    sim_mad_si = mad_si
  ))
}

```

It returns a list of four components:
- `sim_onset`, `sim_diff`, `sim_si`: 
Wasserstein distances for the three main data components.
- `sim_mad_si`: 
The absolute difference in the **mean** serial interval. This provides an
additional, direct constraint on the central tendency of the serial interval,
complementing the distributional comparison from the Wasserstein distance.

We test the `sim_fn` and `scorer_fn` pair to ensure they work correctly with the
observed data.

```{r}

test = tidyabc::test_simulation(
  sim_fn = sim1_fn, 
  scorer_fn = scorer1_fn,
  params = sim_params,
  obsdata = obsdata
  # debug=TRUE
)

# .gg_hist(test$obsdata$onset)

```

## Inference with ABC

We now perform ABC to infer the true parameters from the `obsdata`.

### Priors

We specify priors for the model parameters. The priors for the gamma
distribution hyper-parameters (`mean_*`, `sd_*`) are constrained to be "convex"
(mean > sd), ensuring the distributions have a single mode, which is a
reasonable assumption for biological delays. The prior for `r0` is set to allow
for growth rates consistent with the observation window. The `R0` parameter is
**not given a prior**; it is a deterministic function of `r0`, `mean_gt`, and
`sd_gt`.

```{r}

priors = priors(
  r0 ~ unif(-0.1, 0.7),
  mean_onset ~ unif(0, 12),
  sd_onset ~ unif(0, 8),
  mean_obs ~ unif(0, 12),
  sd_obs ~ unif(0, 8),
  mean_gt ~ unif(0, 12),
  sd_gt ~ unif(0, 8),
  R0 ~ (1+r0*sd_gt^2/mean_gt) ^ (mean_gt^2 / sd_gt^2),
  ~ is.finite(R0) & R0 > 0 & R0 < 12,
  ~ mean_onset > sd_onset,
  ~ mean_obs > sd_obs,
  ~ mean_gt > sd_gt
)

priors
```

### ABC Workflow

We run a multi-stage ABC workflow:

1.  **Initial Rejection Sampling (`abc_rejection`)**:
  - We perform a quick, low-resolution rejection fit with `n_sims=1000` and `acceptance_rate=0.5`.
  - The primary goal is **not** to get the final answer, but to analyze the
  resulting component scores using `posterior_distance_metrics()`. This helps us
  calibrate the `scoreweights` to ensure the serial interval (`sim_si`,
  `sim_mad_si`) has a strong influence on the distance calculation, as it is the
  most informative data for inferring the generation time and R0.



```{r}

abc_fit = abc_rejection(
  obsdata = obsdata,
  priors_list = priors,
  sim_fn = sim1_fn,
  scorer_fn = scorer1_fn,
  n_sims = 1000,
  acceptance_rate = 0.5,
  parallel = TRUE
)

# summary(abc_fit)
metrics = posterior_distance_metrics(abc_fit)

# make the serial interval fitting much more important:
scoreweights1 = metrics$scoreweights 

```

2.  **Sequential Monte Carlo (`abc_smc`)**:
  - Using the calibrated `scoreweights`, we run a more efficient SMC fit with `n_sims=8000`.
  - SMC iteratively refines the proposal distribution, allowing it to home in on
  the high-posterior-density region more effectively than rejection sampling.

```{r}

smc_fit = abc_smc(
  obsdata = obsdata,
  priors_list = priors,
  sim_fn = sim1_fn,
  scorer_fn = scorer1_fn,
  n_sims = 8000,
  acceptance_rate = 0.25,
  #debug_errors = TRUE,
  parallel = TRUE,
  scoreweights = scoreweights1
)

summary(smc_fit)

```

This is generally quite slow for the relative large number of waves and 
simulations it requires until convergence. It has good matching between the 
estimated parameters and the truth for initial growth rate, reproduction number
and observation delays. It is uninformed about delay to onset (and this is 
inherent in the model and data), and the generation time is somewhat constrained
and the mode aligns with the true value but the median is still somewhat high.

```{r}
plot(smc_fit,truth = sim_params)
plot_evolution(smc_fit,truth = sim_params)
```


3.  **Adaptive ABC (`abc_adaptive`)**:
  - Finally, we run the Adaptive ABC algorithm. This method fits empirical
  distributions to the posterior from each wave to create the next proposal,
  which can be very effective for complex, non-Gaussian posteriors.
  - We use `widen_by = 1.2` to provide a safety net against particle degeneracy.


```{r}



adaptive_fit = abc_adaptive(
  obsdata = obsdata,
  priors_list = priors,
  sim_fn = sim1_fn,
  scorer_fn = scorer1_fn,
  n_sims = 4000,
  acceptance_rate = 0.2,
  # debug_errors = TRUE,
  parallel = TRUE,
  scoreweights = scoreweights1,
  widen_by = 1.2
)

summary(adaptive_fit)

```

The adaptive algorithm is quicker, less focussed on exploration and more on 
convergence. With the settings above it can identify the growth rate, reproduction
number, generation time mean, observation delay mean and SD to a high degree of 
accuracy. In this case it tends to generate distributions that are very peaked
but with heavy tails, leading to wide 95% credible intervals even when the 
central estimate seems very close. The model is not informative about the onset
distribution and this affects its predictive ability for the generation time
SD.

```{r}

plot(adaptive_fit,truth = sim_params)

```


The evolution plot shows how the posterior for each parameter evolved over the
adaptive waves, demonstrating the algorithm's convergence.

```{r}
plot_evolution(adaptive_fit,truth = sim_params)
```

A correlation plot accounting for weighting reveals correlations between
parameters in the final posterior (e.g., `r0` and `mean_gt` are often
correlated).

```{r fig.width = 7,fig.height = 7}
plot_correlations(adaptive_fit,truth = sim_params) & ggplot2::theme(
   axis.title.y = ggplot2::element_text(angle=45,vjust=0, hjust=1),
   axis.title.x = ggplot2::element_text(angle=45, hjust=1) #,vjust=1, hjust=0.5)
)
```

- **`plot_convergence(adaptive_fit)`**: 
The key diagnostic for iterative methods, showing the decline in distance
(`abs_distance`), increase in ESS, and stabilization of parameter estimates
(`rel_mean_change`).

```{r}
plot_convergence(adaptive_fit)
```

A powerful posterior predictive check. It generates new simulated datasets from
the posterior and overlays their summary statistics (histograms) on the observed
data. If the model is adequate and the inference successful, the simulated data
should closely match the observed data.

```{r}
plot_simulations(obsdata, adaptive_fit, sim_fn = sim1_fn)
```

### Refining the Priors

With the knowledge that the onset distribution is not informed by the model. We
imagine that we have other data to feed into this model. It is plausible that we
might have independent estimates of symptom onset delay from traveller or
household data. Likewise better estimates of the observation delay may be
available elsewhere. We replace the uniform priors on the delay parameters with
more informative
**log-normal priors** (`lnorm2`) that reflect our prior belief about their
likely scale.

Using the output of previous runs we also given more informed priors for the 
parameters under investigation.

```{r}

priors2 = priors(
  r0 ~ norm(0.18, 0.25),
  mean_onset ~ lnorm2(7, 3),
  sd_onset ~ lnorm2(5, 3),
  mean_obs ~ lnorm2(5, 1),
  sd_obs ~ lnorm2(3, 1),
  # mean_gt ~ lnorm2(4, 3),
  # sd_gt ~ lnorm2(3, 2),
  mean_gt ~ unif(0, 12),
  sd_gt ~ unif(0, 8),
  R0 ~ (1+r0*sd_gt^2/mean_gt) ^ (mean_gt^2 / sd_gt^2),
  ~ is.finite(R0) & R0 > 0 & R0 < 12,
  ~ mean_onset > sd_onset,
  ~ mean_obs > sd_obs,
  ~ mean_gt > sd_gt
)

priors2
```


We run the Adaptive ABC again with these new priors and compare the results.
This allows us to assess the robustness of our inferences to prior
specification. We also want to focus the algorithm on the elements of the data
that are unknown, by modifying the `scoreweights`. We also let the algorithm 
converge hard as we are relatively sure where we are investigating.

```{r}

scoreweights2 = scoreweights1 *
  c(sim_onset = 2, sim_diff = 0.5, sim_si = 2, sim_mad_si = 3)

adaptive_fit2 = abc_adaptive(
  obsdata = obsdata,
  priors_list = priors2,
  sim_fn = sim1_fn,
  scorer_fn = scorer1_fn,
  n_sims = 4000,
  acceptance_rate = 0.2,
  # debug_errors = TRUE,
  parallel = TRUE,
  scoreweights = scoreweights2,
  # widen_by = 1,
  # distfit = "analytical"
)

summary(adaptive_fit2)

```

```{r}

plot(adaptive_fit2,truth = sim_params, tail = 0.01)


# plot_evolution(adaptive_fit2, truth = sim_params, what="proposals")
```

And we can check that posterior resamples from the new fit are still consistent 
with the data. Despite the informed priors the estimate of the SD of the 
generation time is somewhat high, and it is not well informed by the data. This
has mild knock on to the estimate of R0 which is slightly low.

```{r}

plot_simulations(obsdata, adaptive_fit2, sim_fn = sim1_fn)

```

## Conclusion

This vignette showcases the power of `tidyabc` for tackling complex, real-world
inference problems in epidemiology. Including this tricky example where
relatively short generation time is coupled with long delay to symptom onset. By
building a detailed simulation model that captures the hidden processes of
transmission and observation, and by carefully designing the scoring function
and priors, we can use ABC to infer critical but unobservable parameters like R0
and the generation time from limited, biased observational data. The suite of
diagnostic plots provided by `tidyabc` allows for thorough evaluation of the
inference quality and model adequacy.

When working with a real problem developing a simulation first and checking that
the ABC machinery is able to recover the simulation parameters is a very important
aspect to fitting with ABC where there are quite a lot of variables in the
fitting process that all may influence the overall quality of fit.