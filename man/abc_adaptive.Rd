% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abc-workflow.R
\name{abc_adaptive}
\alias{abc_adaptive}
\title{Perform ABC sequential adaptive fitting}
\usage{
abc_adaptive(
  obsdata,
  priors_list,
  sim_fn,
  scorer_fn,
  n_sims,
  acceptance_rate,
  ...,
  max_time = 5 * 60,
  converged_fn = default_termination_fn(),
  obsscores = NULL,
  distance_method = "euclidean",
  seed = NULL,
  knots = NULL,
  parallel = FALSE,
  max_recover = 3,
  allow_continue = interactive(),
  debug_errors = FALSE,
  kernel = "epanechnikov",
  bw = 0.1,
  widen_by = 1.05,
  scoreweights = NULL,
  use_proposal_correlation = TRUE
)
}
\arguments{
\item{obsdata}{The observational data. The data in this will typically
be a named list, but could be anything, e.g. dataframe. It is the reference
data that the simulation model is aiming to replicate.}

\item{priors_list}{a named list of priors specified as a \code{abc_prior} S3 object
(see \code{priors()}), this can include derived values as unnamed 2-sided
formulae, where the LHS of the formula will be assigned to the value of the
RHS, plus optionally a set of constraints as one sided formulae where the
RHS of the formulae will resolve to a boolean value.}

\item{sim_fn}{a user defined function that takes a set of parameters named
the same as \code{priors_list}. It must return a simulated data set in the
same format as \code{obsdata}, or that can be compared to \code{simdata} by
\code{scorer_fn}. This function must not refer to global parameters, and will be
automatically crated with \code{carrier}.}

\item{scorer_fn}{a user supplied function that matches the following
signature \code{scorer_fn(simdata, obsdata, ....)}, i.e. it takes data in the
format of \code{simdata} paired with the original \code{obsdata} and returns a named
list of component scores per simulation. This function can make use of the
\verb{calculate_*()} set of functions to compare components of the simulation to
the original data. This function must not refer to global parameters, and
will be automatically crated with \code{carrier}. If this is a purrr style
function then \code{.x} will refer to simulation output and \code{.y} to original
observation data.}

\item{n_sims}{The number of simulations to run per wave (for SMC and Adaptive)
or overall (for Rejection). For rejection sampling a large number is
recommended, for the others sma}

\item{acceptance_rate}{What proportion of simulations to keep in ABC rejection
or hard ABC parts of the algorithms.}

\item{...}{must be empty}

\item{max_time}{the maximum time in seconds to spend in ABC waves before admitting
defeat. This time may not be all used if the algorithm converges.}

\item{converged_fn}{a function that takes a \code{summary} and \code{per_param} input
and generates a logical indicator that the function has converged}

\item{obsscores}{Summary scores for the observational data. This will
be a named list, and is equivalent to the output of \code{scorer_fn},
on the observed data. If not given typically it will be assumed to be all
zeros.}

\item{distance_method}{what metric is used to combine \code{simscores} and \code{obsscores}.
One of \code{"euclidean"}, \code{"normalised"}, \code{"manhattan"}, or \code{"mahalanobis"}.}

\item{seed}{an optional random seed}

\item{knots}{the number of knots to model the CDF with. Optional, and will be
typically inferred from the data size. Small numbers tend to work better if
we expect the distribution to be unimodal.}

\item{parallel}{parallelise the simulation? If this is set to true then the
simulation step will be parallelised using \code{furrr}. For this to make any
difference it must have been set up with the following:
\code{future::plan(future::multisession, workers = parallel::detectCores()-2)}}

\item{max_recover}{if the effective sample size of SMC or adaptive algorithms
drops below 200, the algorithm will retry the wave with double the sample
size to try and recover the shape of the distribution, up to a maximum of
\code{max_recover} times.}

\item{allow_continue}{if SMC or adaptive algorithms have not converged after
\code{max_time} allow the algorithm to interactively prompt the user to continue.}

\item{debug_errors}{Errors that crop up in \code{sim_fn} during a simulation due
to anomolous value combinations are hard to debug. If this flag is set,
whenever a \code{sim_fn} or \code{scorer_fn} throws an error an interactive debugging
session is started with the failing parameter combinations. This is not
compatible with running in parallel.}

\item{kernel}{one of \code{"epanechnikov"} (default), \code{"uniform"}, \code{"triangular"},
\code{"biweight"}, or \code{"gaussian"}. The kernel defines how the distance metric
translates into the importance weight that decides whether a given
simulation and associated parameters should be rejected or held for the
next round. All kernels except \code{gaussian} have a hard cut-off outside of
which the probability of acceptance of a particle is zero. Use of
\code{gaussian} kernels can result in poor convergence.}

\item{bw}{for Adaptive ABC data distributions are smoothed before modelling
the CDF. Over smoothing can reduce convergence rate, under-smoothing may result
in noisy posterior estimates, and appearance of local modes.
This is a proportion of the ESS and defaults to 0.1.}

\item{widen_by}{change the dispersion of proposal distribution in ABC
adaptive, preserving the median. This is akin to a nonlinear,
heteroscedastic random walk in the quantile space, and can help address
over-fitting or local modes in the ABC adaptive waves. \code{widen_by} is an odds
ratio and describes how much further from the median any given part of the
distribution is after transformation. E.g. if the median of a distribution
is zero, and the \code{widen_by} is 2 then the 0.75 quantile will move to the
position of the 0.9 quantile. The distribution will stay within the
support of the prior. This is by default 1.05 which allows for some
additional variability in proposals.}

\item{scoreweights}{A named vector with names matching output of \code{scorer_fn}
that defines the importance of this component of the scoring in the overall
distance and weighting of any given simulation. This can be used to assign
more weight on certain parts of the model output. For \code{euclidean} and \code{manhattan}
distance methods these weights multiply the output of \code{scorer_fn} directly.
For the other 2 distance methods some degree of normalisation is done first
on the first wave scores to make different components have approximately the
same relevance to the overall score.}

\item{use_proposal_correlation}{When calculating the weight of a particle the
proposal correlation structure is available, to help determine how unusual
or otherwise a particle is.}
}
\value{
an S3 object of class \code{abc_fit} this contains the following:
\itemize{
\item type: the type of ABC algorithm
\item iterations: number of completed iterations
\item converged: boolean - did the result meet convergence criteria
\item waves: a list of dataframes of wave convergence metrics
\item summary: a dataframe with the summary of the parameter fits after each wave.
\item priors: the priors for the fit as a \code{abc_prior} S3 object
\item posteriors: the final wave posteriors
}
}
\description{
This function will execute a simulation for a random selection of parameters.
Based on the \code{acceptance_rate} it will reject a proportion of the results.
The remaining results are weighted (using a kernel with a tolerance
equivalent to half the acceptance rate). Empirical distributions are fitted
to weighted parameter particles and from these proposals are generated for
further waves by fresh sampling. Waves are executed until a maximum is
reached or the results converge sufficiently that the changes between waves
are small. A relatively small number of simulations may be attempted with a
high acceptance rate, over multiple waves.
}
\details{
Performs the ABC Adaptive algorithm.
This iterative method refines parameter estimates across waves by fitting
empirical proposal distributions to the weighted posterior samples from the
previous wave. Unlike ABC-SMC, which uses a fixed perturbation kernel,
\code{abc_adaptive} constructs a new proposal distribution \eqn{Q_t(\theta)} at
each wave \eqn{t}.
\enumerate{
\item \strong{Initialization (Wave 1):}
Parameters \eqn{\theta^{(i)}} are sampled from the prior \eqn{P(\theta)}.
Simulations are run, summary statistics \eqn{S_s^{(i)}} are computed,
and distances \eqn{d^{(i)} = d(S_s^{(i)}, S_o)} are calculated.
A tolerance threshold \eqn{\epsilon_1} is set as the
\eqn{\alpha = \texttt{acceptance\_rate}} quantile of these distances.
Unnormalized weights \eqn{\tilde{w}^{(i)}_1} are calculated using a kernel
\eqn{K_{\epsilon_1}(d^{(i)})}.
\item \strong{Subsequent Waves (\eqn{t > 1}):}
\itemize{
\item \strong{Proposal Generation:} An empirical joint proposal distribution
\eqn{Q_t(\theta)} is constructed from the weighted posterior sample
\eqn{\{(\theta^{(i)}_{t-1}, w^{(i)}_{t-1})\}} of the previous wave.
This is done by fitting marginal empirical distributions
\eqn{Q_{t,j}(\theta_j)} to each parameter \eqn{\theta_j}, using the
\code{empirical()} function with the prior \eqn{P_j(\theta_j)} as a link to
enforce support constraints. These marginals are assumed independent,
but their weighted correlation matrix \eqn{R_t} is retained as an
attribute and used to induce dependence in the MVN sampling space.
New proposals \eqn{\theta^{(i)}_t} are generated by:
\itemize{
\item Sampling a vector \eqn{Z \sim \mathcal{N}(0, R_t)} in a
correlated standard normal space.
\item Mapping each component \eqn{Z_j} to uniform space:
\eqn{U_j = \Phi(Z_j)}.
\item Mapping to the parameter space using the empirical quantile
functions: \eqn{\theta^{(i)}_{t,j} = Q_{t,j}^{-1}(U_j)}.
}
\item \strong{Simulation and Weighting:} Simulations are run for the new proposals.
Distances \eqn{d^{(i)}_t} are computed and the tolerance \eqn{\epsilon_t}
is set as the \eqn{\alpha}-quantile of the current wave's distances.
The unnormalized weight for particle \eqn{i} in wave \eqn{t} is calculated as:
\deqn{
       \tilde{w}^{(i)}_t = \frac{P(\theta^{(i)}_t) K_{\epsilon_t}(d^{(i)}_t)}{Q_t(\theta^{(i)}_t)}
     }
where \eqn{P(\theta^{(i)}_t) = \prod_j P_j(\theta^{(i)}_{t,j})} is the prior
density (assuming independence), \eqn{K_{\epsilon_t}} is the ABC kernel,
and \eqn{Q_t(\theta^{(i)}_t) = \prod_j Q_{t,j}(\theta^{(i)}_{t,j})} is the
empirical proposal density (also assuming independence for density
calculation, consistent with the marginal fitting). The correlation
structure is handled in the sampling process, and optionally in the density
evaluation.
}
\item \strong{Normalization:} Weights \eqn{w^{(i)}_t} are normalized to sum to one.
The algorithm includes a recovery mechanism: if the Effective Sample Size
(ESS) falls below a threshold (e.g., 200), the acceptance rate is
increased (i.e., \eqn{\epsilon_t} is made larger) to accept more particles
and improve the ESS.
\item \strong{Termination:} The process repeats until a maximum time is reached or
convergence criteria based on parameter stability and credible interval
contraction are met.
}
}
\examples{

fit = abc_adaptive(
  obsdata = example_obsdata(),
  priors_list = example_priors_list(),
  sim_fn = example_sim_fn,
  scorer_fn = example_scorer_fn,
  n_sims = 1000,
  acceptance_rate = 0.25,
  max_time = 5, # 5 seconds to fit within examples limit
  parallel = FALSE,
  allow_continue = FALSE
)

summary(fit)

}
\concept{workflow}
