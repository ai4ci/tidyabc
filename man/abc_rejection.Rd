% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abc-workflow.R
\name{abc_rejection}
\alias{abc_rejection}
\title{Perfom simple ABC rejection algorithm}
\usage{
abc_rejection(
  obsdata,
  priors_list,
  sim_fn,
  scorer_fn,
  n_sims,
  acceptance_rate,
  ...,
  converged_fn = default_termination_fn(),
  obsscores = NULL,
  distance_method = "euclidean",
  keep_simulations = FALSE,
  seed = NULL,
  parallel = FALSE,
  debug_errors = FALSE,
  kernel = "epanechnikov",
  scoreweights = NULL
)
}
\arguments{
\item{obsdata}{The observational data. The data in this will typically
be a named list, but could be anything, e.g. dataframe. It is the reference
data that the simulation model is aiming to replicate.}

\item{priors_list}{a named list of priors specified as a \code{abc_prior} S3 object
(see \code{priors()}), this can include derived values as unnamed 2-sided
formulae, where the LHS of the formula will be assigned to the value of the
RHS, plus optionally a set of constraints as one sided formulae where the
RHS of the formulae will resolve to a boolean value.}

\item{sim_fn}{a user defined function that takes a set of parameters named
the same as \code{priors_list}. It must return a simulated data set in the
same format as \code{obsdata}, or that can be compared to \code{simdata} by
\code{scorer_fn}. This function must not refer to global parameters, and will be
automatically crated with \code{carrier}.}

\item{scorer_fn}{a user supplied function that matches the following
signature \code{scorer_fn(simdata, obsdata, ....)}, i.e. it takes data in the
format of \code{simdata} paired with the original \code{obsdata} and returns a named
list of component scores per simulation. This function can make use of the
\verb{calculate_*()} set of functions to compare components of the simulation to
the original data. This function must not refer to global parameters, and
will be automatically crated with \code{carrier}. If this is a purrr style
function then \code{.x} will refer to simulation output and \code{.y} to original
observation data.}

\item{n_sims}{The number of simulations to run per wave (for SMC and Adaptive)
or overall (for Rejection). For rejection sampling a large number is
recommended, for the others sma}

\item{acceptance_rate}{What proportion of simulations to keep in ABC rejection
or hard ABC parts of the algorithms.}

\item{...}{must be empty}

\item{converged_fn}{a function that takes a \code{summary} and \code{per_param} input
and generates a logical indicator that the function has converged}

\item{obsscores}{Summary scores for the observational data. This will
be a named list, and is equivalent to the output of \code{scorer_fn},
on the observed data. If not given typically it will be assumed to be all
zeros.}

\item{distance_method}{what metric is used to combine \code{simscores} and \code{obsscores}
and is one of \code{"euclidean"}, \code{"manhattan"}, or \code{"mahalanobis"}.}

\item{keep_simulations}{keep the individual simulation results in the output
of an ABC workflow. This can have large implications for the size of the
result. It may also not be what you want and it is probably worth considering
resampling the posteriors rather than keeping the simulations.}

\item{seed}{an optional random seed}

\item{parallel}{parallelise the simulation? If this is set to true then the
simulation step will be parallelised using \code{furrr}. For this to make any
difference it must have been set up with the following:
\code{future::plan(future::multisession, workers = parallel::detectCores()-2)}}

\item{debug_errors}{Errors that crop up in \code{sim_fn} during a simulation due
to anomolous value combinations are hard to debug. If this flag is set,
whenever a \code{sim_fn} or \code{scorer_fn} throws an error an interactive debugging
session is started with the failing parameter combinations. This is not
compatible with running in parallel.}

\item{kernel}{one of \code{"epanechnikov"}, \code{"uniform"}, \code{"triangular"}, \code{"biweight"},
or \code{"gaussian"}. The kernel defines how the distance metric translates into
the importance weight that decides whether a given simulation and associated
parameters should be rejected or held for the next round.}

\item{scoreweights}{A named vector with names matching output of \code{scorer_fn}
that defines the importance of this component of the scoring in the overall
distance and weighting of any given simulation. This can be used to assign
more weight on certain parts of the model output.}
}
\value{
an S3 object of class \code{abc_fit} this contains the following:
\itemize{
\item type: the type of ABC algorithm
\item iterations: number of completed iterations
\item converged: boolean - did the result meet convergence criteria
\item waves: a list of dataframes of wave convergence metrics
\item summary: a dataframe with the summary of the parameter fits after each wave.
\item priors: the priors for the fit as a \code{abc_prior} S3 object
\item posteriors: the final wave posteriors
}
}
\description{
This function will execute a simulation for a random selection of parameters
and identify the best matching \code{acceptance_rate} percent, as defined by the
summary distance metric. A large number of simulations and a low acceptance
rate are best here.
}
\examples{

fit = abc_rejection(
  example_obsdata(),
  example_priors_list(),
  example_sim_fn,
  example_scorer_fn,
  n_sims = 10000,
  acceptance_rate = 0.01
)

summary(fit)

}
\concept{workflow}
