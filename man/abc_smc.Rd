% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abc-workflow.R
\name{abc_smc}
\alias{abc_smc}
\title{Perform ABC sequential Monte Carlo fitting}
\usage{
abc_smc(
  obsdata,
  priors_list,
  sim_fn,
  scorer_fn,
  n_sims,
  acceptance_rate,
  ...,
  max_time = 5 * 60,
  converged_fn = default_termination_fn(),
  obsscores = NULL,
  distance_method = "euclidean",
  seed = NULL,
  parallel = FALSE,
  allow_continue = interactive(),
  debug_errors = FALSE,
  kernel = "epanechnikov",
  scoreweights = NULL
)
}
\arguments{
\item{obsdata}{The observational data. The data in this will typically
be a named list, but could be anything, e.g. dataframe. It is the reference
data that the simulation model is aiming to replicate.}

\item{priors_list}{a named list of priors specified as a \code{abc_prior} S3 object
(see \code{priors()}), this can include derived values as unnamed 2-sided
formulae, where the LHS of the formula will be assigned to the value of the
RHS, plus optionally a set of constraints as one sided formulae where the
RHS of the formulae will resolve to a boolean value.}

\item{sim_fn}{a user defined function that takes a set of parameters named
the same as \code{priors_list}. It must return a simulated data set in the
same format as \code{obsdata}, or that can be compared to \code{simdata} by
\code{scorer_fn}. This function must not refer to global parameters, and will be
automatically crated with \code{carrier}.}

\item{scorer_fn}{a user supplied function that matches the following
signature \code{scorer_fn(simdata, obsdata, ....)}, i.e. it takes data in the
format of \code{simdata} paired with the original \code{obsdata} and returns a named
list of component scores per simulation. This function can make use of the
\verb{calculate_*()} set of functions to compare components of the simulation to
the original data. This function must not refer to global parameters, and
will be automatically crated with \code{carrier}. If this is a purrr style
function then \code{.x} will refer to simulation output and \code{.y} to original
observation data.}

\item{n_sims}{The number of simulations to run per wave (for SMC and Adaptive)
or overall (for Rejection). For rejection sampling a large number is
recommended, for the others sma}

\item{acceptance_rate}{What proportion of simulations to keep in ABC rejection
or hard ABC parts of the algorithms.}

\item{...}{must be empty}

\item{max_time}{the maximum time in seconds to spend in ABC waves before admitting
defeat. This time may not be all used if the algorithm converges.}

\item{converged_fn}{a function that takes a \code{summary} and \code{per_param} input
and generates a logical indicator that the function has converged}

\item{obsscores}{Summary scores for the observational data. This will
be a named list, and is equivalent to the output of \code{scorer_fn},
on the observed data. If not given typically it will be assumed to be all
zeros.}

\item{distance_method}{what metric is used to combine \code{simscores} and \code{obsscores}.
One of \code{"euclidean"}, \code{"normalised"}, \code{"manhattan"}, or \code{"mahalanobis"}.}

\item{seed}{an optional random seed}

\item{parallel}{parallelise the simulation? If this is set to true then the
simulation step will be parallelised using \code{furrr}. For this to make any
difference it must have been set up with the following:
\code{future::plan(future::multisession, workers = parallel::detectCores()-2)}}

\item{allow_continue}{if SMC or adaptive algorithms have not converged after
\code{max_time} allow the algorithm to interactively prompt the user to continue.}

\item{debug_errors}{Errors that crop up in \code{sim_fn} during a simulation due
to anomolous value combinations are hard to debug. If this flag is set,
whenever a \code{sim_fn} or \code{scorer_fn} throws an error an interactive debugging
session is started with the failing parameter combinations. This is not
compatible with running in parallel.}

\item{kernel}{one of \code{"epanechnikov"} (default), \code{"uniform"}, \code{"triangular"},
\code{"biweight"}, or \code{"gaussian"}. The kernel defines how the distance metric
translates into the importance weight that decides whether a given
simulation and associated parameters should be rejected or held for the
next round. All kernels except \code{gaussian} have a hard cut-off outside of
which the probability of acceptance of a particle is zero. Use of
\code{gaussian} kernels can result in poor convergence.}

\item{scoreweights}{A named vector with names matching output of \code{scorer_fn}
that defines the importance of this component of the scoring in the overall
distance and weighting of any given simulation. This can be used to assign
more weight on certain parts of the model output. For \code{euclidean} and \code{manhattan}
distance methods these weights multiply the output of \code{scorer_fn} directly.
For the other 2 distance methods some degree of normalisation is done first
on the first wave scores to make different components have approximately the
same relevance to the overall score.}
}
\value{
an S3 object of class \code{abc_fit} this contains the following:
\itemize{
\item type: the type of ABC algorithm
\item iterations: number of completed iterations
\item converged: boolean - did the result meet convergence criteria
\item waves: a list of dataframes of wave convergence metrics
\item summary: a dataframe with the summary of the parameter fits after each wave.
\item priors: the priors for the fit as a \code{abc_prior} S3 object
\item posteriors: the final wave posteriors
}
}
\description{
This function will execute a simulation for a random selection of parameters.
Based on the \code{acceptance_rate} it will reject a proportion of the results.
The remaining results are weighted (using a kernel with a tolerance
equivalent to half the acceptance rate). Weighted parameter particles
generate proposals for further waves but a particle perturbation. Waves
are executed until a maximum is reached or the results converge sufficiently
that the changes between waves are small. A relatively small number of
simulations may be attempted with a high acceptance rate, over multiple waves.
}
\details{
Performs the ABC Sequential Monte Carlo (SMC) algorithm.
This iterative method refines parameter estimates across multiple waves.
\enumerate{
\item \strong{Initialization (Wave 1):}
Parameters \eqn{\theta^{(i)}} are sampled from the prior \eqn{P(\theta)}.
Simulations \eqn{D_s^{(i)} = M(\theta^{(i)})} are run, summaries \eqn{S_s^{(i)}}
are computed, and distances \eqn{d^{(i)} = d(S_s^{(i)}, S_o)} are calculated.
A tolerance threshold \eqn{\epsilon_1} is set as the
\eqn{\alpha = \texttt{acceptance\_rate}} quantile of these initial distances.
Unnormalized weights \eqn{\tilde{w}^{(i)}_1} are calculated using a kernel
\eqn{K_{\epsilon_1}(d^{(i)})}.
\item \strong{Subsequent Waves (\eqn{t > 1}):}
\itemize{
\item \strong{Proposal Generation:} A particle \eqn{\theta^{(j)}_{t-1}} is selected
from the previous wave's accepted particles with probability proportional
to its weight \eqn{w^{(j)}_{t-1}}. The particle is then perturbed in a
transformed MVN space using a multivariate normal kernel with covariance
\eqn{\Sigma_t = \frac{\kappa_t^2}{d} \text{Cov}_{w_{t-1}}(\theta_{t-1})},
where \eqn{\text{Cov}_{w_{t-1}}} is the weighted covariance from wave \eqn{t-1},
\eqn{d} is the parameter dimension, and \eqn{\kappa_t} is the \code{kernel_t} parameter.
The new proposal \eqn{\theta^{(i)}_t} is generated as:
\deqn{
       \theta^{(i)}_t = \theta^{(j)}_{t-1} + \zeta, \quad \zeta \sim \mathcal{N}(0, \Sigma_t)
     }
This proposal is mapped back to the original parameter space using the
prior's copula transformation (from MVN space defined by prior CDFs).
\item \strong{Simulation and Weighting:} Simulations \eqn{D_s^{(i)} = M(\theta^{(i)}_t)}
are run for the new proposals. Distances \eqn{d^{(i)}_t} are computed.
The tolerance \eqn{\epsilon_t} is set as the \eqn{\alpha}-quantile of
distances from the \emph{current} wave's simulations. The unnormalized weight
for particle \eqn{i} in wave \eqn{t} is calculated as:
\deqn{
       \tilde{w}^{(i)}_t = \frac{P(\theta^{(i)}_t) K_{\epsilon_t}(d^{(i)}_t)}{q_t(\theta^{(i)}_t)}
     }
where \eqn{P(\theta^{(i)}_t)} is the prior density, \eqn{K_{\epsilon_t}}
is the ABC kernel, and \eqn{q_t(\theta^{(i)}_t)} is the proposal density
from the previous wave's weighted particles (calculated using the
perturbation kernel). This proposal density is computed as a weighted sum:
\deqn{
       q_t(\theta^{(i)}_t) = \sum_j w^{(j)}_{t-1} \phi(\theta^{(i)}_t; \theta^{(j)}_{t-1}, \Sigma_t)
     }
where \eqn{\phi(\cdot; \mu, \Sigma)} is the PDF of a multivariate normal
with mean \eqn{\mu} and covariance \eqn{\Sigma}.
}
\item \strong{Normalization:} Weights \eqn{w^{(i)}_t} are normalized to sum to one.
Particles with negligible weights are typically filtered out.
\item \strong{Termination:} The process repeats until a maximum number of waves or
time is reached, or convergence criteria are met based on changes in
parameter estimates or effective sample size (ESS).
}
}
\examples{

fit = abc_smc(
  obsdata = example_obsdata(),
  priors_list = example_priors_list(),
  sim_fn = example_sim_fn,
  scorer_fn = example_scorer_fn,
  n_sims = 1000,
  acceptance_rate = 0.25,
  max_time = 5, # 5 seconds to fit within examples limit
  parallel = FALSE,
  allow_continue = FALSE
)

summary(fit)

}
\concept{workflow}
